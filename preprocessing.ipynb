{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from seewhence.emoji import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sarcasm Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_path = '/home/ben/data/acl_2020/reddit.jsonl'\n",
    "twitter_path = '/home/ben/data/acl_2020/twitter.jsonl'\n",
    "\n",
    "with jsonlines.open(twitter_path) as twitter_reader:\n",
    "    _twitter = pd.DataFrame([x for x in twitter_reader])\n",
    "    _twitter['source'] = 'twitter'\n",
    "\n",
    "with jsonlines.open(reddit_path) as reddit_reader:\n",
    "    _reddit = pd.DataFrame([x for x in reddit_reader])\n",
    "    _reddit['source'] = 'reddit'\n",
    "\n",
    "# add multiindex for hygiene\n",
    "# df = pd.concat([_twitter,_reddit], keys=['twitter','reddit'])\n",
    "\n",
    "df = pd.concat([_twitter,_reddit]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make classification easier with binary\n",
    "df['label'] = df.label.apply(lambda x: 1 if x == 'SARCASM' else 0)\n",
    "df['context_0'] = df.apply(lambda x: x.context[0], axis=1)\n",
    "df['context_1'] = df.apply(lambda x: x.context[1], axis=1)\n",
    "df['combined'] = df.apply(lambda x: '\\n'.join(x.context + [x.response]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPQTiLH6XsP4"
   },
   "source": [
    "## Deal with Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-LE7BE4blQj"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DO_EMOJI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ecaafb4227c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mDO_EMOJI\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtext_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'context_0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'context_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_emoji_unicode'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_emojis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DO_EMOJI' is not defined"
     ]
    }
   ],
   "source": [
    "text_cols = ['response','context_0','context_1']\n",
    "\n",
    "for col in text_cols:\n",
    "  df[col+'_emoji_unicode'] = df[col].apply(lambda x : ' '.join(get_emojis(x)))\n",
    "  df[col+'_emoji_string'] = df[col].apply(lambda x : ' '.join(clean_emojis(translate_emojis(x))))\n",
    "\n",
    "emoji_text_cols = [col for col in df.columns if 'emoji_string' in col]\n",
    "emoji_cols = [col for col in df.columns if 'emoji_unicode' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "hhymap1KkUXd",
    "outputId": "58557fb2-0ca8-4c1e-e731-bb88317c70ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from seewhence import models\n",
    "from seewhence import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8 if not train_split else train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(df) * train_split)\n",
    "test_size = int(len(df) - train_size)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "twitter = df.loc[df.source == 'twitter']\n",
    "reddit = df.loc[df.source == 'reddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([twitter[:train_size//2],\n",
    "                   reddit[:train_size//2]])\n",
    "valid = pd.concat([twitter[train_size//2:train_size//2+test_size//2],\n",
    "                   reddit[train_size//2:train_size//2+test_size//2]])\n",
    "test = pd.concat([twitter[-(test_size//2):],\n",
    "                   reddit[-(test_size//2):]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUTPUT_DIR + 'df.csv')\n",
    "train.to_csv(OUTPUT_DIR + 'train.csv')\n",
    "valid.to_csv(OUTPUT_DIR + 'valid.csv')\n",
    "test.to_csv(OUTPUT_DIR + 'test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
